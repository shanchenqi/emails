  oss-security - Linux kernel: oob-write in  drivers/vhost/net.c:get_rx_bufs()            Products  Openwall GNU/*/Linux   server OS Linux Kernel Runtime Guard John the Ripper   password cracker  Free & Open Source for any platform in the cloud Pro for Linux Pro for macOS  Wordlists   for password cracking passwdqc   policy enforcement  Free & Open Source for Unix Pro for Windows (Active Directory)  yescrypt   KDF & password hashing yespower   Proof-of-Work (PoW) crypt_blowfish   password hashing phpass   ditto in PHP tcb   better password shadowing Pluggable Authentication Modules scanlogd   port scan detector popa3d   tiny POP3 daemon blists   web interface to mailing lists msulogin   single user mode login php_mt_seed   mt_rand() cracker  Services Publications  Articles Presentations  Resources  Mailing lists Community wiki Source code repositories (GitHub) Source code repositories (CVSweb) File archive & mirrors How to verify digital signatures OVE IDs  What's new         Follow @Openwall on Twitter for new release announcements and other news   [<prev] [next>] [day] [month] [year] [list]  Date: Fri, 25 Jan (EST) From: Vladis Dronov <vdronov@...hat.com> To: oss-security@...ts.openwall.com Subject: Linux kernel: oob-write in  drivers/vhost/net.c:get_rx_bufs()  Hello,  A flaw was found in the Linux kernel in the handle_rx() function in the [vhost_net] driver. A malicious virtual guest under specific conditions can trigger an out-of-bounds write in a kmalloc-8 slab on a virtual host which may lead to a kernel memory corruption and a system panic. Due to the nature of the flaw, privilege escalation cannot be fully ruled out, although we believe it is unlikely.  Reference:  An id was assigned to this flaw and we would like to ask to use it in the related public communications.  This flaw was found and researched by Jason Wang, Red Hat Inc.  The suggested patch is below. Please, note, this is preliminary internal patch which was not reviewed and accepted by the upstream community. The researcher is going to send the patch to the related kernel mailing lists.  A reproducer can be provided via direct off-list request to me.  Please, see some details extracted from discussion with a researcher:  > is this guest triggerable (guest -> host) or host -> host?  a vm guest can trigger an oob-write on a host but requires a large network packet to be received for this.  > what is overwritten?  kmalloc-8 slab on a vm host.  > what's the minimum and maximum size of the out-of-bound write?  from 8 bytes (sizeof vring_used_elem) to bytes (63 * sizeof(vring_used_elem))  > does the attacker control the data that are written and if yes, to which degree?  attacker can not directly control the data.  Best regards, Vladis Dronov | Red Hat, Inc. | Product Security | Senior Software Engineer  ===[ ]=== From: Jason Wang <jasowang@...hat.com> Subject: [PATCH] vhost: fix OOB in get_rx_bufs()  After batched used ring updating was introduced in commit ("vhost_net: batch used ring update in rx"). We tend to batch heads in vq->heads for more than one packet. But the quota passed to get_rx_bufs() was not correctly limited, which can result a OOB write in vq->heads.          headcount = get_rx_bufs(vq, vq->heads + nvq->done_idx,                     vhost_len, &in, vq_log, &log,                     likely(mergeable) ? UIO_MAXIOV : 1);  UIO_MAXIOV was still used which is wrong since we could have batched used in vq->heads, this will cause OOB if the next buffer needs more than (UIO_MAXIOV) - 64 (VHOST_NET_BATCH)) heads after we've batched 64 (VHOST_NET_BATCH) heads:  ============================================================================= BUG kmalloc-8k (Tainted: G    B            ): Redzone overwritten -----------------------------------------------------------------------------  INFO: First byte 0xa9 instead of 0xcc INFO: Allocated in cpu=2                                                     INFO: Slab objects=3 used=3 fp=0x          (null) INFO: Object  Fixing this by allocating UIO_MAXIOV + VHOST_NET_BATCH iovs for vhost-net. This is done through set the limitation through vhost_dev_init(), then set_owner can allocate the number of iov in a per device manner.  Fixes: ("vhost_net: batch used ring update in rx") Signed-off-by: Jason Wang <jasowang@...hat.com> ---  drivers/vhost/net.c   | 3 ++-  drivers/vhost/scsi.c  | 2 +-  drivers/vhost/vhost.c | 7 ++++---  drivers/vhost/vhost.h | 4 +++-  drivers/vhost/vsock.c | 2 +-  5 files changed, 11 insertions(+), 7 deletions(-)  diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c index --- a/drivers/vhost/net.c +++ b/drivers/vhost/net.c @@ @@ static int vhost_net_open(struct inode *inode, struct file *f)  		n->vqs[i].rx_ring = NULL;  		vhost_net_buf_init(&n->vqs[i].rxq);  	} -	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX); +	vhost_dev_init(dev, vqs, VHOST_NET_VQ_MAX, +		       UIO_MAXIOV + VHOST_NET_BATCH);    	vhost_poll_init(n->poll + VHOST_NET_VQ_TX, handle_tx_net, EPOLLOUT, dev);  	vhost_poll_init(n->poll + VHOST_NET_VQ_RX, handle_rx_net, EPOLLIN, dev); diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c index --- a/drivers/vhost/scsi.c +++ b/drivers/vhost/scsi.c @@ @@ static int vhost_scsi_open(struct inode *inode, struct file *f)  		vqs[i] = &vs->vqs[i].vq;  		vs->vqs[i].vq.handle_kick = vhost_scsi_handle_kick;  	} -	vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ); +	vhost_dev_init(&vs->dev, vqs, VHOST_SCSI_MAX_VQ, UIO_MAXIOV);    	vhost_scsi_init_inflight(vs, NULL);   diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c index --- a/drivers/vhost/vhost.c +++ b/drivers/vhost/vhost.c @@ @@ static long vhost_dev_alloc_iovecs(struct vhost_dev *dev)  		vq->indirect = kmalloc_array(UIO_MAXIOV,  					     sizeof(*vq->indirect),  					     GFP_KERNEL); -		vq->log = kmalloc_array(UIO_MAXIOV, sizeof(*vq->log), +		vq->log = kmalloc_array(dev->iov_limit, sizeof(*vq->log),  					GFP_KERNEL); -		vq->heads = kmalloc_array(UIO_MAXIOV, sizeof(*vq->heads), +		vq->heads = kmalloc_array(dev->iov_limit, sizeof(*vq->heads),  					  GFP_KERNEL);  		if (!vq->indirect || !vq->log || !vq->heads)  			goto err_nomem; @@ @@ static void vhost_dev_free_iovecs(struct vhost_dev *dev)  }    void vhost_dev_init(struct vhost_dev *dev, -		    struct vhost_virtqueue **vqs, int nvqs) +		    struct vhost_virtqueue **vqs, int nvqs, int iov_limit)  {  	struct vhost_virtqueue *vq;  	int i; @@ @@ void vhost_dev_init(struct vhost_dev *dev,  	dev->iotlb = NULL;  	dev->mm = NULL;  	dev->worker = NULL; +	dev->iov_limit = iov_limit;  	init_llist_head(&dev->work_list);  	init_waitqueue_head(&dev->wait);  	INIT_LIST_HEAD(&dev->read_list); diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h index --- a/drivers/vhost/vhost.h +++ b/drivers/vhost/vhost.h @@ @@ struct vhost_dev {  	struct list_head read_list;  	struct list_head pending_list;  	wait_queue_head_t wait; +	int iov_limit;  };   -void vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, int nvqs); +void vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, +		    int nvqs, int iov_limit);  long vhost_dev_set_owner(struct vhost_dev *dev);  bool vhost_dev_has_owner(struct vhost_dev *dev);  long vhost_dev_check_owner(struct vhost_dev *); diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c index --- a/drivers/vhost/vsock.c +++ b/drivers/vhost/vsock.c @@ @@ static int vhost_vsock_dev_open(struct inode *inode, struct file *file)  	vsock->vqs[VSOCK_VQ_TX].handle_kick = vhost_vsock_handle_tx_kick;  	vsock->vqs[VSOCK_VQ_RX].handle_kick = vhost_vsock_handle_rx_kick;   -	vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs)); +	vhost_dev_init(&vsock->dev, vqs, ARRAY_SIZE(vsock->vqs), UIO_MAXIOV);    	file->private_data = vsock;  	spin_lock_init(&vsock->send_pkt_list_lock); --   Powered by blists - more mailing lists  Please check out the  Open Source Software Security Wiki, which is counterpart to this mailing list.  Confused about mailing lists and their use? Read about mailing lists on Wikipedia and check out these guidelines on proper formatting of your messages.      